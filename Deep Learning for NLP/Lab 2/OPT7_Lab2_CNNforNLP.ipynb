{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "528S9T9NoLMT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aV0mvCTtoLMX"
   },
   "source": [
    "# Deep Learning for NLP - lab exercise 2\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "The data can be download here: http://teaching.caio-corro.fr/2019-2020/OPT7/imdb.zip\n",
    "\n",
    "There are two files: one with positive reviews (imdb.pos) and one with negative reviews (imdb.neg). Each file contains 300000 reviews, one per line.\n",
    "\n",
    "\n",
    "The following functions can be used to load and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40h2dzGIoLMY"
   },
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(f.readline()).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD57n_U-oLMa"
   },
   "source": [
    "The following cell load the first 5000 sentences in each review set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "vW-R101uoLMb",
    "outputId": "82a8be34-1802-4e6c-e701-fd38c7dc8569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  ./Data/imdb.pos  /  1  lines discarded\n",
      "Load  5000  lines from  ./Data/imdb.neg  /  1  lines discarded\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile = \"./Data/imdb.pos\"  # path of the file containing positive reviews\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "\n",
    "txtfile = \"./Data/imdb.neg\"  # path of the file containing negative reviews\n",
    "negtxt = loadTexts(txtfile,limit=LIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['do', \"n't\", 'miss', 'it', 'if', 'you', 'can'],\n",
       " ['dreams', 'of', 'a', 'young', 'girl'],\n",
       " ['funny', 'funny', 'movie', '!'],\n",
       " ['pride', 'and', 'prejudice', 'is', 'absolutely', 'amazing', '!', '!'],\n",
       " ['quirky', 'and', 'effective'],\n",
       " ['mike', 'leigh', \"'s\", 'best', 'and', 'the', 'best', 'of', '2010'],\n",
       " ['an', 'experience', 'unmatched', 'in', 'film'],\n",
       " ['if',\n",
       "  'john',\n",
       "  'woo',\n",
       "  'were',\n",
       "  'to',\n",
       "  'of',\n",
       "  'filmed',\n",
       "  'the',\n",
       "  'wizard',\n",
       "  'of',\n",
       "  'oz',\n",
       "  'on',\n",
       "  'the',\n",
       "  'set',\n",
       "  'of',\n",
       "  'the',\n",
       "  'wild',\n",
       "  'bunch'],\n",
       " ['it',\n",
       "  'has',\n",
       "  'its',\n",
       "  'shortcomings',\n",
       "  ',',\n",
       "  'and',\n",
       "  'i',\n",
       "  'presume',\n",
       "  'the',\n",
       "  'book',\n",
       "  'would',\n",
       "  'be',\n",
       "  'much',\n",
       "  'better',\n",
       "  'but',\n",
       "  'it',\n",
       "  'is',\n",
       "  'still',\n",
       "  'well',\n",
       "  'worth',\n",
       "  'watching'],\n",
       " ['csi', 'meets', 'the', 'x', 'files']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postxt[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uELwatrBoLMe"
   },
   "source": [
    "Split the data between train / dev / test, for example by creating lists txt_train, label_train, txt_dev, ... You should take care to keep a 50/50 ratio between positive and negative instances in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "rmhoi67FoLMe",
    "outputId": "0c423e5c-6318-443d-eb1f-4b471365ff31"
   },
   "outputs": [],
   "source": [
    "# A label of 1 means that the review is positive, 0 means negative\n",
    "\n",
    "label_pos = [1. for i in range(len(postxt))]\n",
    "label_neg = [0. for i in range(len(negtxt))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = postxt + negtxt\n",
    "labels = label_pos + label_neg\n",
    "\n",
    "test_size = 0.2\n",
    "dev_size = 0.2\n",
    "\n",
    "# Split off test set\n",
    "X_traindev, X_test, y_traindev, y_test = train_test_split(trainset,\n",
    "                                                         labels,\n",
    "                                                         test_size=test_size,\n",
    "                                                         random_state=42,\n",
    "                                                         stratify=labels)\n",
    "# Divide leftover data in train and dev set\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_traindev,\n",
    "                                                  y_traindev,\n",
    "                                                  test_size=dev_size/(1-test_size),\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_traindev)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['far', 'and', 'away', ',', 'the', 'best', 'of', 'the', 'draculas', '!'],\n",
       " ['hg', 'wells', 'in', 'name', 'alone'],\n",
       " ['horrible'],\n",
       " ['what', 'the', 'hell', '!', '!', '!'],\n",
       " ['not',\n",
       "  'quite',\n",
       "  'a',\n",
       "  'classic',\n",
       "  ',',\n",
       "  'but',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'watch',\n",
       "  'all',\n",
       "  'the',\n",
       "  'same'],\n",
       " ['avatar', 'pocahontas', 'in', 'space'],\n",
       " ['blah', 'fest', '2003'],\n",
       " ['worst', 'cooper', 'movie', 'ever'],\n",
       " ['boring', ',', 'inaccurate', ',', 'uninspired'],\n",
       " ['a', 'great', 'hammer', 'film', 'even', 'without', 'christopher', 'lee']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KFVFcEHoLMh"
   },
   "source": [
    "# Converting data to Pytorch tensors\n",
    "\n",
    "We will first convert data to Pytorch tensors so they can be used in a neural network.\n",
    "To do that, you must first create a dictionnary that will map words to integers.\n",
    "Add to the dictionnary only words that are in the training set (be sure to understand why we do that!).\n",
    "\n",
    "Then, you can convert the data to tensors:\n",
    "- use tensors of longs: both the sentence and the label will be represented as integers, not floats!\n",
    "- these tensors do not require a gradient\n",
    "\n",
    "A tensor representing a sentence is composed of the integer representation of each word, e.g. [10, 256, 3, 4].\n",
    "Note that some words in the dev and test sets may not be in the dictionnary! (i.e. unknown words)\n",
    "You can just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAOuLi1uoLMj"
   },
   "outputs": [],
   "source": [
    "vocab_size = 0\n",
    "dictionary = {}\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        if X_train[i][j] not in dictionary:\n",
    "            dictionary[X_train[i][j]] = vocab_size\n",
    "            vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line):\n",
    "    return th.LongTensor([dictionary[word] for word in line if word in dictionary])\n",
    "\n",
    "def lines_to_tensors(data):\n",
    "    max_len = max(len(line) for line in data)\n",
    "    tensors = th.LongTensor(np.zeros((len(data), max_len), dtype='int'))\n",
    "    for i in range(len(data)):\n",
    "        tensor = line_to_tensor(data[i])\n",
    "        tensors[i, :len(tensor)] = tensor\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's handiest to already have all the sentences in the dataset transformed to tensors so we can have a cleaner training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tnsr, X_dev_tnsr, X_test_tnsr = lines_to_tensors(X_train), lines_to_tensors(X_dev), lines_to_tensors(X_test)\n",
    "\n",
    "# The labels need to be transformed into tensors, which is what the following code does\n",
    "y_train = th.Tensor(np.array(y_train))\n",
    "y_dev = th.Tensor(np.array(y_dev))\n",
    "y_test = th.Tensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ...,    0,    0,    0],\n",
      "        [   9,   10,   11,  ...,    0,    0,    0],\n",
      "        [  14,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5947,    0,    0,  ...,    0,    0,    0],\n",
      "        [5948,  795,    0,  ...,    0,    0,    0],\n",
      "        [   5,  390,  391,  ...,    0,    0,    0]]) 41\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tnsr, len(X_train_tnsr[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the maximum length of a sentence in the training set is 41, thus all the tensors will have this length. The sentences that are shorter are transformed into a tensor with zeros padded at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amsFT3yroLMq"
   },
   "source": [
    "# Neural network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_layer1DBatched(nn.Module):\n",
    "    def __init__(self, emb_dim, window_size, vocab_size, n_filters, device):\n",
    "        super(CNN_layer1DBatched, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.n_filters = n_filters\n",
    "        self.device = device\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        self.conv1 = nn.Linear(window_size * emb_dim, n_filters)\n",
    "        \n",
    "        fulconn_layers = []\n",
    "        fulconn_layers.append(nn.ReLU())\n",
    "        fulconn_layers.append(nn.Linear(n_filters, 1))\n",
    "        self.fulconn = nn.Sequential(*fulconn_layers)\n",
    "        \n",
    "    def sliding_window(self, embs, n_window_steps, output_tensor):\n",
    "        '''\n",
    "        This function takes the following inputs:\n",
    "        1. Sentence embeddings (torch vector)\n",
    "        2. n_window_steps, or sentence length divided by window_size divided by\n",
    "        one.\n",
    "        3. The tensor to store the outputs in.\n",
    "        '''\n",
    "        for step in range(n_window_steps):\n",
    "            embs_before_concat = [embs[:, step+i, :] for i in range(self.window_size)]\n",
    "            concat_array = th.cat(embs_before_concat, axis=1)\n",
    "            output_tensor[:, :, step] = self.conv1(concat_array)\n",
    "        return output_tensor\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        n_window_steps = inputs.shape[1] - self.window_size + 1\n",
    "        conv_output = th.zeros(inputs.shape[0], self.n_filters, n_window_steps)\n",
    "        embs = self.embeddings(inputs)\n",
    "        conv_output = self.sliding_window(embs, n_window_steps, conv_output)\n",
    "        pooling_layer = nn.MaxPool1d(kernel_size = n_window_steps)\n",
    "        out = F.relu(pooling_layer(conv_output))\n",
    "        out = self.fulconn((out.reshape(inputs.shape[0],-1)).to(self.device)).reshape((-1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rG4KAZ8hoLMt"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "Create a loss function builder.\n",
    "\n",
    "- Pytorch loss functions are documented here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "- In our case, we are interested in *BCELoss* and *BCEWithLogitsLoss*. Read their documentation and choose the one that fits with your network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0XVoEAaoLMt"
   },
   "outputs": [],
   "source": [
    "BCELogitLoss = th.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-W4BaYHoLMw"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Write your training loop!\n",
    "\n",
    "- parameterizable number of epochs\n",
    "- at each epoch, print the mean loss and the dev accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainingLoopClassification():\n",
    "    def __init__(self, model, optimizer, loss, accuracy, n_epochs, batch_size, gradient_clip, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        '''\n",
    "        This function initializes the weights and biases of all the layers in the\n",
    "        network. This function automatically only initializes the nn.Linear type\n",
    "        objects due to the if-statement check. It uses kaiming initialization and\n",
    "        it initializes the biases with zeros.\n",
    "        '''\n",
    "        \n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.kaiming_uniform_(m.weight.data) \n",
    "            nn.init.zeros_(m.bias.data)   \n",
    "        \n",
    "    def train(self, train_data, train_labels, dev_data, dev_labels):\n",
    "        self.model.apply(self.init_weights)\n",
    "        \n",
    "        loss_list = []\n",
    "        dev_acc_list = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            print('Starting epoch: {}'.format(epoch))\n",
    "            self.model.train()\n",
    "            cost = 0\n",
    "            \n",
    "            for first in range(0, len(train_data), batch_size):\n",
    "                self.model.zero_grad()                                     \n",
    "                \n",
    "                batch_input = th.cat(\n",
    "                    [\n",
    "                        sentence.reshape(1, -1)\n",
    "                        for sentence in train_data[first:first + batch_size]\n",
    "                        \n",
    "                    ],\n",
    "                        dim=0\n",
    "                ).to(self.device)\n",
    "                batch_labels = train_labels[first:first + batch_size].to(self.device)\n",
    "                \n",
    "                output = self.model(batch_input)\n",
    "                                                \n",
    "                loss = self.loss(output, batch_labels)\n",
    "                                \n",
    "                loss.backward()\n",
    "                \n",
    "                th.nn.utils.clip_grad_value_(self.model.parameters(), self.gradient_clip)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                cost += loss.item()\n",
    "                \n",
    "            mean_loss = cost / (len(train_data)/batch_size+1)\n",
    "            acc = self.accuracy(self.model, dev_data, dev_labels, self.device)\n",
    "            \n",
    "            print('mean loss: ', mean_loss)\n",
    "            print('dev accuracy: ', acc)\n",
    "            \n",
    "            loss_list.append(mean_loss)\n",
    "            dev_acc_list.append(acc)\n",
    "        \n",
    "        return self.model, loss_list, dev_acc_list\n",
    "    \n",
    "    def plot_graphs(self,\n",
    "                    mean_losss, \n",
    "                    dev_accus, \n",
    "                    embedding_size, \n",
    "                    n_hidden_layers):\n",
    "        plt.plot([i for i in range(EPOCHS)],mean_losss, label='mean loss')\n",
    "        plt.plot([i for i in range(EPOCHS)],dev_accus, label='accuracy on dev')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title('{} hidden layers, {} embedding_size'.format(n_hidden_layers,\n",
    "                                                              embedding_size))\n",
    "        plt.legend()\n",
    "        plt.savefig('{}HiddenLayer{}.png'.format(n_hidden_layers,\n",
    "                                                embedding_size))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model,\n",
    "             X, \n",
    "             y,\n",
    "            device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, label in zip(X, y):\n",
    "        x = data\n",
    "        if x.size()[0] <= 0:\n",
    "            continue\n",
    "        output = model(x.reshape(1, -1).to(device))\n",
    "        total += 1\n",
    "        if (output >= 0) == (label == 1.):\n",
    "            correct += 1\n",
    "    return correct/total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPO985z3oLNO",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0\n",
      "mean loss:  1.0118235447678705\n",
      "dev accuracy:  52.800000000000004\n",
      "Starting epoch: 1\n",
      "mean loss:  0.7267079283767417\n",
      "dev accuracy:  56.75\n",
      "Starting epoch: 2\n",
      "mean loss:  0.6767485669816521\n",
      "dev accuracy:  59.099999999999994\n",
      "Starting epoch: 3\n",
      "mean loss:  0.6442383276372753\n",
      "dev accuracy:  60.650000000000006\n",
      "Starting epoch: 4\n",
      "mean loss:  0.6170584455092959\n",
      "dev accuracy:  62.0\n",
      "Starting epoch: 5\n",
      "mean loss:  0.5921059342531058\n",
      "dev accuracy:  62.8\n",
      "Starting epoch: 6\n",
      "mean loss:  0.5683864853110174\n",
      "dev accuracy:  63.4\n",
      "Starting epoch: 7\n",
      "mean loss:  0.5451755930005081\n",
      "dev accuracy:  64.35\n",
      "Starting epoch: 8\n",
      "mean loss:  0.5223627639069798\n",
      "dev accuracy:  64.9\n",
      "Starting epoch: 9\n",
      "mean loss:  0.4999270685787859\n",
      "dev accuracy:  65.85\n",
      "Starting epoch: 10\n",
      "mean loss:  0.47810708670148166\n",
      "dev accuracy:  66.60000000000001\n",
      "Starting epoch: 11\n",
      "mean loss:  0.4566439964094592\n",
      "dev accuracy:  66.60000000000001\n",
      "Starting epoch: 12\n",
      "mean loss:  0.435531118029941\n",
      "dev accuracy:  66.85\n",
      "Starting epoch: 13\n",
      "mean loss:  0.4147160663528847\n",
      "dev accuracy:  67.55\n",
      "Starting epoch: 14\n",
      "mean loss:  0.39442839143446967\n",
      "dev accuracy:  68.30000000000001\n",
      "Starting epoch: 15\n",
      "mean loss:  0.37444045641694207\n",
      "dev accuracy:  68.95\n",
      "Starting epoch: 16\n",
      "mean loss:  0.35503650652318797\n",
      "dev accuracy:  69.35\n",
      "Starting epoch: 17\n",
      "mean loss:  0.33618815713599126\n",
      "dev accuracy:  69.65\n",
      "Starting epoch: 18\n",
      "mean loss:  0.3179553736900461\n",
      "dev accuracy:  69.6\n",
      "Starting epoch: 19\n",
      "mean loss:  0.30056957525030686\n",
      "dev accuracy:  70.1\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 10\n",
    "window_size = 2\n",
    "vocab_size = vocab_size\n",
    "n_filters = 10\n",
    "device = th.device('cuda')\n",
    "    \n",
    "model = CNN_layer1DBatched(embedding_size, window_size, vocab_size, n_filters, device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = th.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "loss = BCELogitLoss\n",
    "accuracy = accuracy\n",
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "gradient_clip = 5.\n",
    "\n",
    "models_list = []\n",
    "mean_loss_list = []\n",
    "dev_accuracy_list = []\n",
    "\n",
    "training_loop = trainingLoopClassification(model, optimizer, loss, accuracy, n_epochs, batch_size, gradient_clip, device)\n",
    "\n",
    "model, mean_losses, dev_accus = training_loop.train(X_train_tnsr, y_train, X_dev_tnsr, y_dev)\n",
    "\n",
    "models_list.append(model)\n",
    "mean_loss_list.append(mean_losses)\n",
    "dev_accuracy_list.append(dev_accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = accuracy(model, X_test_tnsr, y_test, device)\n",
    "test_accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "opt7_lab_exercise_1_petit_devatine.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
