{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMI2 project classification comparison\n",
    "\n",
    "This project comprises a comparison between several classifiers over a few datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "\n",
    "PATH='Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column contains the labels, which are separated from the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_array[:,0:8]\n",
    "y = df_array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers tested are the following:\n",
    "\n",
    "- Decision Tree with limited depth (depth 5)\n",
    "- 5-Nearest Neighbour\n",
    "- Gaussian Naive Bayes\n",
    "- Support Vector Machine with radial basis kernel\n",
    "- Linear Support Vector Machine\n",
    "- AdaBoost\n",
    "- Bagging (Decision tree with unlimited depth)\n",
    "- Random Forest (depth 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_test_split` from scikit-learn is used to split the dataset in a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_diabetes = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_diabetes.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 0.7489177489177489),\n",
       " ('KNN', 0.6926406926406926),\n",
       " ('GNB', 0.7445887445887446),\n",
       " ('SVMRBF', 0.7489177489177489),\n",
       " ('SVMLIN', 0.7489177489177489),\n",
       " ('ADA', 0.7402597402597403),\n",
       " ('BAG', 0.696969696969697),\n",
       " ('RFC', 0.7186147186147186)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_diabetes = list(zip(names_clf, score_diabetes))\n",
    "results_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible from the results above, the models all perform rather similar. Still, there are some models that perform better than the rest, with the Decision Tree coming out on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['data']\n",
    "y = df['target']\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_iris = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_iris.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 1.0),\n",
       " ('KNN', 1.0),\n",
       " ('GNB', 0.9777777777777777),\n",
       " ('SVMRBF', 1.0),\n",
       " ('SVMLIN', 0.9777777777777777),\n",
       " ('ADA', 1.0),\n",
       " ('BAG', 1.0),\n",
       " ('RFC', 1.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_iris = list(zip(names_clf, score_iris))\n",
    "results_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible from the results above, many models achieve a perfect score. Only the Naive Bayes and Linear SVM did not do a perfect job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast-cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['data']\n",
    "y = df['target']\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_cancer = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_cancer.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 0.9473684210526315),\n",
       " ('KNN', 0.9590643274853801),\n",
       " ('GNB', 0.935672514619883),\n",
       " ('SVMRBF', 0.9707602339181286),\n",
       " ('SVMLIN', 0.9766081871345029),\n",
       " ('ADA', 0.9766081871345029),\n",
       " ('BAG', 0.9649122807017544),\n",
       " ('RFC', 0.9649122807017544)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cancer = list(zip(names_clf, score_cancer))\n",
    "results_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the models all seem to do well on this dataset. The Linear SVM and AdaBoost both achieve the same level of performance, with other models being not far behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial dataset\n",
    "\n",
    "Multiple artificial classification problems are generated with different numbers of classes.\n",
    "\n",
    "First up is a dataset with two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_classification(n_samples=1000, n_classes=2,n_informative=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[0]\n",
    "y = df[1]\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_2 = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_2.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 0.7566666666666667),\n",
       " ('KNN', 0.8),\n",
       " ('GNB', 0.7),\n",
       " ('SVMRBF', 0.84),\n",
       " ('SVMLIN', 0.7766666666666666),\n",
       " ('ADA', 0.7866666666666666),\n",
       " ('BAG', 0.82),\n",
       " ('RFC', 0.6666666666666666)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2 = list(zip(names_clf, score_2))\n",
    "results_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is a dataset with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_classification(n_samples=1000, n_classes=10, n_informative=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[0]\n",
    "y = df[1]\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_10 = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_10.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 0.36666666666666664),\n",
       " ('KNN', 0.33),\n",
       " ('GNB', 0.32666666666666666),\n",
       " ('SVMRBF', 0.4033333333333333),\n",
       " ('SVMLIN', 0.4033333333333333),\n",
       " ('ADA', 0.25666666666666665),\n",
       " ('BAG', 0.4033333333333333),\n",
       " ('RFC', 0.26)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_10 = list(zip(names_clf, score_10))\n",
    "results_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a dataset with 25 classes is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_classification(n_samples=1000, n_classes=25, n_informative=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[0]\n",
    "y = df[1]\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_clf = [\"DT\", \"KNN\", \"GNB\", \"SVMRBF\", \"SVMLIN\", \"ADA\", \"BAG\", \"RFC\"]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(max_depth=5),\n",
    "               KNeighborsClassifier(),\n",
    "               GaussianNB(),\n",
    "               SVC(),\n",
    "               SVC(kernel='linear'),\n",
    "               AdaBoostClassifier(),\n",
    "               BaggingClassifier(),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Development\\Anaconda3.7\\envs\\OPT8DL\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "score_25 = []\n",
    "\n",
    "for name, clf in zip(names_clf, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score_25.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 0.15666666666666668),\n",
       " ('KNN', 0.10666666666666667),\n",
       " ('GNB', 0.13333333333333333),\n",
       " ('SVMRBF', 0.15),\n",
       " ('SVMLIN', 0.14666666666666667),\n",
       " ('ADA', 0.08333333333333333),\n",
       " ('BAG', 0.16333333333333333),\n",
       " ('RFC', 0.07666666666666666)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_25 = list(zip(names_clf, score_25))\n",
    "results_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Friedman test and the Nemenyi post-hoc test are part of the projects requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FriedmanchisquareResult(statistic=39.14285714285714, pvalue=2.2226749280281907e-07)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.friedmanchisquare(score_diabetes, score_iris, score_cancer,\n",
    "                             score_2, score_10, score_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray([(score_diabetes[0], score_iris[0], score_cancer[0], score_2[0], score_10[0], score_25[0]),\n",
    "                  (score_diabetes[1], score_iris[1], score_cancer[1], score_2[1], score_10[1], score_25[1]),\n",
    "                  (score_diabetes[2], score_iris[2], score_cancer[2], score_2[2], score_10[2], score_25[2]),\n",
    "                  (score_diabetes[3], score_iris[3], score_cancer[3], score_2[3], score_10[3], score_25[3]),\n",
    "                  (score_diabetes[4], score_iris[4], score_cancer[4], score_2[4], score_10[4], score_25[4]),\n",
    "                  (score_diabetes[5], score_iris[5], score_cancer[5], score_2[5], score_10[5], score_25[5]),\n",
    "                  (score_diabetes[6], score_iris[6], score_cancer[6], score_2[6], score_10[6], score_25[6]),\n",
    "                  (score_diabetes[7], score_iris[7], score_cancer[7], score_2[7], score_10[7], score_25[7])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  4.916667\n",
      "1  3.750000\n",
      "2  2.750000\n",
      "3  6.583333\n",
      "4  5.333333\n",
      "5  4.166667\n",
      "6  5.666667\n",
      "7  2.833333\n",
      "     0         1         2         3         4         5         6         7\n",
      "0  1.0  0.991762  0.790194  0.938200  0.999991  0.999504  0.999504  0.821971\n",
      "1  1.0  1.000000  0.996822  0.479397  0.952767  0.999991  0.877288  0.998171\n",
      "2  1.0  1.000000  1.000000  0.119256  0.601719  0.974328  0.439611  1.000000\n",
      "3  1.0  1.000000  1.000000  1.000000  0.987549  0.681547  0.998171  0.137778\n",
      "4  1.0  1.000000  1.000000  1.000000  1.000000  0.991762  0.999998  0.642096\n",
      "5  1.0  1.000000  1.000000  1.000000  1.000000  1.000000  0.964730  0.981837\n",
      "6  1.0  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  0.479397\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/ptasheq/ceb29503fbc16b048bb121684f7fe7dc\n",
    "\n",
    "from scipy.stats import friedmanchisquare, rankdata, norm\n",
    "from scipy.special import gammaln\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# consistent with https://cran.r-project.org/web/packages/PMCMR/vignettes/PMCMR.pdf p. 17\n",
    "def test_nemenyi():\n",
    "    data = np.asarray([(3.88, 5.64, 5.76, 4.25, 5.91, 4.33), (30.58, 30.14, 16.92, 23.19, 26.74, 10.91),\n",
    "                       (25.24, 33.52, 25.45, 18.85, 20.45, 26.67), (4.44, 7.94, 4.04, 4.4, 4.23, 4.36),\n",
    "                       (29.41, 30.72, 32.92, 28.23, 23.35, 12), (38.87, 33.12, 39.15, 28.06, 38.23, 26.65)])\n",
    "    print(friedmanchisquare(data[0], data[1], data[2], data[3], data[4], data[5]))\n",
    "    nemenyi = NemenyiTestPostHoc(data)\n",
    "    meanRanks, pValues = nemenyi.do()\n",
    "    print(meanRanks)\n",
    "    print(pValues)\n",
    "\n",
    "\n",
    "class NemenyiTestPostHoc():\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self._noOfGroups = data.shape[0]\n",
    "        self._noOfSamples = data.shape[1]\n",
    "        self._data = data\n",
    "\n",
    "    def do(self):\n",
    "        dataAsRanks = np.full(self._data.shape, np.nan)\n",
    "        for i in range(self._noOfSamples):\n",
    "            dataAsRanks[:, i] = rankdata(self._data[:, i])\n",
    "        meansOfRanksOfDependentSamples = np.mean(dataAsRanks, 1)\n",
    "        qValues = self._compareStatisticsOfAllPairs(meansOfRanksOfDependentSamples)\n",
    "        pValues = self._calculatePValues(qValues)\n",
    "\n",
    "        return meansOfRanksOfDependentSamples, pValues\n",
    "\n",
    "    def _compareStatisticsOfAllPairs(self, meansOfRanks):\n",
    "        noOfMeansOfRanks = len(meansOfRanks)\n",
    "        compareResults = np.zeros((noOfMeansOfRanks-1, noOfMeansOfRanks))\n",
    "        for i in range(noOfMeansOfRanks-1):\n",
    "            for j in range(i+1, noOfMeansOfRanks):\n",
    "                compareResults[i][j] = self._compareStatisticsOfSinglePair((meansOfRanks[i], meansOfRanks[j]))\n",
    "        return compareResults\n",
    "\n",
    "    def _compareStatisticsOfSinglePair(self, meansOfRanksPair):\n",
    "        diff = abs(meansOfRanksPair[0] - meansOfRanksPair[1])\n",
    "        qval = diff / np.sqrt(self._noOfGroups * (self._noOfGroups + 1) / (6 * self._noOfSamples))\n",
    "        return qval * np.sqrt(2)\n",
    "\n",
    "    def _calculatePValues(self, qValues):\n",
    "        for qRow in qValues:\n",
    "            for i in range(len(qRow)):\n",
    "                qRow[i] = self._ptukey(qRow[i], 1, self._noOfGroups, np.inf)\n",
    "        return 1 - qValues\n",
    "\n",
    "    def _wprob(self, w, rr, cc):\n",
    "        nleg = 12\n",
    "        ihalf = 6\n",
    "\n",
    "        C1 = -30\n",
    "        C2 = -50\n",
    "        C3 = 60\n",
    "        M_1_SQRT_2PI = 1 / np.sqrt(2 * np.pi)\n",
    "        bb = 8\n",
    "        wlar = 3\n",
    "        wincr1 = 2\n",
    "        wincr2 = 3\n",
    "        xleg = [\n",
    "            0.981560634246719250690549090149,\n",
    "            0.904117256370474856678465866119,\n",
    "            0.769902674194304687036893833213,\n",
    "            0.587317954286617447296702418941,\n",
    "            0.367831498998180193752691536644,\n",
    "            0.125233408511468915472441369464\n",
    "        ]\n",
    "        aleg = [\n",
    "            0.047175336386511827194615961485,\n",
    "            0.106939325995318430960254718194,\n",
    "            0.160078328543346226334652529543,\n",
    "            0.203167426723065921749064455810,\n",
    "            0.233492536538354808760849898925,\n",
    "            0.249147045813402785000562436043\n",
    "        ]\n",
    "\n",
    "        qsqz = w * 0.5\n",
    "\n",
    "        if qsqz >= bb:\n",
    "            return 1.0\n",
    "\n",
    "        # find (f(w/2) - 1) ^ cc\n",
    "        # (first term in integral of hartley's form).\n",
    "\n",
    "        pr_w = 2 * norm.cdf(qsqz) - 1\n",
    "        if pr_w >= np.exp(C2 / cc):\n",
    "            pr_w = pr_w ** cc\n",
    "        else:\n",
    "            pr_w = 0.0\n",
    "\n",
    "        # if w is large then the second component of the\n",
    "        # integral is small, so fewer intervals are needed.\n",
    "\n",
    "        wincr = wincr1 if w > wlar else wincr2\n",
    "\n",
    "        # find the integral of second term of hartley's form\n",
    "        # for the integral of the range for equal-length\n",
    "        # intervals using legendre quadrature.  limits of\n",
    "        # integration are from (w/2, 8).  two or three\n",
    "        # equal-length intervals are used.\n",
    "\n",
    "        # blb and bub are lower and upper limits of integration.\n",
    "\n",
    "        blb = qsqz\n",
    "        binc = (bb - qsqz) / wincr\n",
    "        bub = blb + binc\n",
    "        einsum = 0.0\n",
    "\n",
    "        # integrate over each interval\n",
    "\n",
    "        cc1 = cc - 1.0\n",
    "        for wi in range(1, wincr + 1):\n",
    "            elsum = 0.0\n",
    "            a = 0.5 * (bub + blb)\n",
    "\n",
    "            # legendre quadrature with order = nleg\n",
    "\n",
    "            b = 0.5 * (bub - blb)\n",
    "\n",
    "            for jj in range(1, nleg + 1):\n",
    "                if (ihalf < jj):\n",
    "                    j = (nleg - jj) + 1\n",
    "                    xx = xleg[j-1]\n",
    "                else:\n",
    "                    j = jj\n",
    "                    xx = -xleg[j-1]\n",
    "                c = b * xx\n",
    "                ac = a + c\n",
    "\n",
    "                # if exp(-qexpo/2) < 9e-14\n",
    "                # then doesn't contribute to integral\n",
    "\n",
    "                qexpo = ac * ac\n",
    "                if qexpo > C3:\n",
    "                    break\n",
    "\n",
    "                pplus = 2 * norm.cdf(ac)\n",
    "                pminus = 2 * norm.cdf(ac, w)\n",
    "\n",
    "                # if rinsum ^ (cc-1) < 9e-14, */\n",
    "                # then doesn't contribute to integral */\n",
    "\n",
    "                rinsum = (pplus * 0.5) - (pminus * 0.5)\n",
    "                if (rinsum >= np.exp(C1 / cc1)):\n",
    "                    rinsum = (aleg[j-1] * np.exp(-(0.5 * qexpo))) * (rinsum ** cc1)\n",
    "                    elsum += rinsum\n",
    "\n",
    "            elsum *= (((2.0 * b) * cc) * M_1_SQRT_2PI)\n",
    "            einsum += elsum\n",
    "            blb = bub\n",
    "            bub += binc\n",
    "\n",
    "        # if pr_w ^ rr < 9e-14, then return 0\n",
    "        pr_w += einsum\n",
    "        if pr_w <= np.exp(C1 / rr):\n",
    "            return 0\n",
    "\n",
    "        pr_w = pr_w ** rr\n",
    "        if (pr_w >= 1):\n",
    "            return 1\n",
    "        return pr_w\n",
    "\n",
    "    def _ptukey(self, q, rr, cc, df):\n",
    "\n",
    "        M_LN2 = 0.69314718055994530942\n",
    "\n",
    "        nlegq = 16\n",
    "        ihalfq = 8\n",
    "\n",
    "        eps1 = -30.0\n",
    "        eps2 = 1.0e-14\n",
    "        dhaf = 100.0\n",
    "        dquar = 800.0\n",
    "        deigh = 5000.0\n",
    "        dlarg = 25000.0\n",
    "        ulen1 = 1.0\n",
    "        ulen2 = 0.5\n",
    "        ulen3 = 0.25\n",
    "        ulen4 = 0.125\n",
    "        xlegq = [\n",
    "            0.989400934991649932596154173450,\n",
    "            0.944575023073232576077988415535,\n",
    "            0.865631202387831743880467897712,\n",
    "            0.755404408355003033895101194847,\n",
    "            0.617876244402643748446671764049,\n",
    "            0.458016777657227386342419442984,\n",
    "            0.281603550779258913230460501460,\n",
    "            0.950125098376374401853193354250e-1\n",
    "        ]\n",
    "        alegq = [\n",
    "            0.271524594117540948517805724560e-1,\n",
    "            0.622535239386478928628438369944e-1,\n",
    "            0.951585116824927848099251076022e-1,\n",
    "            0.124628971255533872052476282192,\n",
    "            0.149595988816576732081501730547,\n",
    "            0.169156519395002538189312079030,\n",
    "            0.182603415044923588866763667969,\n",
    "            0.189450610455068496285396723208\n",
    "        ]\n",
    "\n",
    "        if q <= 0:\n",
    "            return 0\n",
    "\n",
    "        if (df < 2) or (rr < 1) or (cc < 2):\n",
    "            return float('nan')\n",
    "\n",
    "        if np.isfinite(q) is False:\n",
    "            return 1\n",
    "\n",
    "        if df > dlarg:\n",
    "            return self._wprob(q, rr, cc)\n",
    "\n",
    "        # in fact we don't need the code below and majority of variables:\n",
    "\n",
    "        # calculate leading constant\n",
    "\n",
    "        f2 = df * 0.5\n",
    "        f2lf = ((f2 * np.log(df)) - (df * M_LN2)) - gammaln(f2)\n",
    "        f21 = f2 - 1.0\n",
    "\n",
    "        # integral is divided into unit, half-unit, quarter-unit, or\n",
    "        # eighth-unit length intervals depending on the value of the\n",
    "        # degrees of freedom.\n",
    "\n",
    "        ff4 = df * 0.25\n",
    "        if df <= dhaf:\n",
    "            ulen = ulen1\n",
    "        elif df <= dquar:\n",
    "            ulen = ulen2\n",
    "        elif df <= deigh:\n",
    "            ulen = ulen3\n",
    "        else:\n",
    "            ulen = ulen4\n",
    "\n",
    "        f2lf += np.log(ulen)\n",
    "\n",
    "        ans = 0.0\n",
    "\n",
    "        for i in range(1, 51):\n",
    "            otsum = 0.0\n",
    "\n",
    "            # legendre quadrature with order = nlegq\n",
    "            # nodes (stored in xlegq) are symmetric around zero.\n",
    "\n",
    "            twa1 = (2*i - 1) * ulen\n",
    "\n",
    "            for jj in range(1, nlegq + 1):\n",
    "                if (ihalfq < jj):\n",
    "                    j = jj - ihalfq - 1\n",
    "                    t1 = (f2lf + (f21 * np.log(twa1 + (xlegq[j] * ulen)))) - (((xlegq[j] * ulen) + twa1) * ff4)\n",
    "                else:\n",
    "                    j = jj - 1\n",
    "                    t1 = (f2lf + (f21 * np.log(twa1 - (xlegq[j] * ulen)))) + (((xlegq[j] * ulen) - twa1) * ff4)\n",
    "\n",
    "                # if exp(t1) < 9e-14, then doesn't contribute to integral\n",
    "                if t1 >= eps1:\n",
    "                    if ihalfq < jj:\n",
    "                        qsqz = q * np.sqrt(((xlegq[j] * ulen) + twa1) * 0.5)\n",
    "                    else:\n",
    "                        qsqz = q * np.sqrt(((-(xlegq[j] * ulen)) + twa1) * 0.5)\n",
    "\n",
    "                    wprb = self._wprob(qsqz, rr, cc)\n",
    "                    rotsum = (wprb * alegq[j]) * np.exp(t1)\n",
    "                    otsum += rotsum\n",
    "\n",
    "            # if integral for interval i < 1e-14, then stop.\n",
    "            # However, in order to avoid small area under left tail,\n",
    "            # at least  1 / ulen  intervals are calculated.\n",
    "\n",
    "            if (i * ulen >= 1.0) and (otsum <= eps2):\n",
    "                break\n",
    "\n",
    "            ans += otsum\n",
    "\n",
    "        return min(1, ans)\n",
    "\n",
    "\n",
    "nemenyi = NemenyiTestPostHoc(data)\n",
    "meanRanks, pValues = nemenyi.do()\n",
    "print(pd.DataFrame(meanRanks))\n",
    "print(pd.DataFrame(pValues))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
